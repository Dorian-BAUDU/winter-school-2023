{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./logo-pinocchio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pinocchio: rigib-body derivatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import magic_donotload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "More than anywhere in Pinocchio, the derivative algorithms are the results of the hard work of Justin Carpentier. Read more about the mathematics behing the code in *Carpentier and Mansard, \"Analytical derivatives of rigid body dynamics algorithms\", RSS 2018*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up\n",
    "We will need Pinocchio, the robot models stored in the package `example-robot-data`, a viewer (either GepettoViewer or MeshCat), some basic linear-algebra operators and the SciPy optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinocchio as pin\n",
    "import example_robot_data as robex\n",
    "import numpy as np\n",
    "from numpy.linalg import inv, pinv, eig, norm, svd, det\n",
    "from scipy.optimize import fmin_bfgs\n",
    "import time\n",
    "import copy\n",
    "np.set_printoptions(precision=2, linewidth=200, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick description of the tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow the same roadmap as for the previous tutorial, and compute the derivatives of each cost function. We then re-start with a manipulator robot, that has a regular vector configuration (no fancy Lie group there in the begining), and only consider free-basis robot at the very end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot = robex.load(\"talos_arm\")   # Load a 6-dof manipulator arm\n",
    "\n",
    "Viewer = pin.visualize.MeshcatVisualizer\n",
    "\n",
    "viz = Viewer(robot.model, robot.collision_model, robot.visual_model)\n",
    "viz.initViewer(loadModel=True)\n",
    "viz.display(robot.q0)\n",
    "\n",
    "# jupyter_cell does not like to be inside a if/then block\n",
    "isinstance(viz, pin.visualize.MeshcatVisualizer) and viz.viewer.jupyter_cell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmodel = robot.model\n",
    "rdata = rmodel.createData()\n",
    "\n",
    "# Arbitrarily selects a frame (and the parent joint) for later cost functions.\n",
    "frame_index = rmodel.getFrameId('gripper_left_fingertip_1_link')\n",
    "joint_index = rmodel.frames[frame_index].parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Derivatives of the 6d \"placement\" cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first study the derivatives of the 6d cost. As Pinocchio works with spatial \"6d\" quantities, this derivative is indeed slightly more intuitive that the one of the 3d cost.\n",
    "\n",
    "**Notations**: For the derivations that follows, let denote by $\\ ^oM_*$ the target placement `Mtarget` and by $\\ ^oM_e(q)$ the placement of the operational frame `rdata.oMf[frame_index]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recall first that the 6d cost function is the log of the relative placement $cost(q) = log(^oM_{*}^{-1} \\ ^oM_e(q))$, with $\\ ^oM_{*}$ a fixed placement, and $\\ ^oM_e(q)$ the placement of a given operational frame $\\mathcal{F}_e$ of the robot. Applying [the chain rule](https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/v/chain-rule-introduction), the derivative of $cost$ must be the product of two derivatives: the derivative of $log$, and the derivative of the relative placement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a copy of the code explained in the first notebook:\n",
    "q = pin.randomConfiguration(rmodel)\n",
    "Mtarget = pin.SE3(pin.utils.rotate('x', np.pi / 4), np.array([0.5, 0.1, 0.27]))  # arbitrary values\n",
    "pin.forwardKinematics(rmodel, rdata, q)\n",
    "Meff = rdata.oMf[frame_index]\n",
    "targetMeff = Mtarget.inverse() * Meff\n",
    "residual = pin.log(targetMeff).vector\n",
    "cost = sum(residual ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative of the placement of a frame attached to the robot\n",
    "#### Spatial velocities\n",
    "The derivative of a place $\\ ^AM_B(q)$ is a spatial velocity, denoted $\\nu_{AB}$. The spatial velocity is the representation of the vector field of 3D velocity of each point of the rigid body. In Pinocchio, $\\nu$ can be represented in two ways:\n",
    "- either in the $\\mathcal{F}_B$ frame, $\\ ^B\\nu_{AB} = (\\ ^Bv_B, \\ ^B\\omega)$. In that case both the linear part $v$ and the angular part $\\omega$ are represented in the $B$ frame, while $v_B$ is the velocity of the center of the frame $\\mathcal{F}_B$.\n",
    "- or in the $\\mathcal{F}_A$ frame, $\\ ^A\\nu_{AB} = (\\ ^Av_A, \\ ^A\\omega)$. In that case, $v$ and $\\omega$ are expressed along the frames of $\\mathcal{F}_A$, and $v_A$ is the velocity of the point rigidly attached to the body passing through the centre of $\\mathcal{F}_A$ at this instant. \n",
    "\n",
    "Spatial velocities are implemented in Pinocchio by the class `pin.Motion`, and are respectively argument and output of `pin.exp` and `pin.log`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joint jacobians\n",
    "When the spatial velocity of a frame attached to the robot is the consequence of a joint velocity $v_q$, it can be computed as $\\nu = J(q) v_q$. As we said, $\\nu$ can be represented in an arbitrary frame. The two logical choices are either the `pin.WORLD` frame, i.e. the frame attached to the universe joint `rmodel.joints[0]`; or it can be the local joint attached to the frame we are observing.\n",
    "\n",
    "Similarly, the Jacobian should be expressed in either of these two frames $\\ ^o\\nu = \\ ^oJ(q) v_q$ or $\\ ^e\\nu = \\ ^eJ(q) v_q$ (where $\\ ^oe_E(q)$ is  operational frame of interest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pin.computeJointJacobians(rmodel, rdata, q)   # precomputes all jacobians\n",
    "oJ = pin.getJointJacobian(rmodel, rdata, joint_index, pin.WORLD)  # in world frame\n",
    "eJ = pin.getJointJacobian(rmodel, rdata, joint_index, pin.LOCAL)  # in local frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shortcut `pin.computeJacobian(rmodel, rdata, q, joint_index)` computes a single jacobian, without pre-calculation, but only in the local frame (as running this version of the algorithm in the world frame is not efficient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frame jacocians\n",
    "We yet gave the syntax for evaluating the jacobian of a frame attached to a joint. The syntax is quite similar for frame jacobians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pin.computeJointJacobians(rmodel, rdata, q)  # precomputes all jacobians\n",
    "oJf = pin.getFrameJacobian(rmodel, rdata, frame_index, pin.WORLD)  # in world frame\n",
    "fJf = pin.getFrameJacobian(rmodel, rdata, frame_index, pin.LOCAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, as the operational frame and the joint space are rigidly attached, their velocity vector fields are the same, hence the expression of their spatial velocity in a same frame are equals, hence their world jacobians are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert norm(oJf - oJ) == 0  # no numerical rounding errors here, they are exactly the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing the frame of expression of velocities\n",
    "If we want to expressed the spatial velocity in another frame, we can move its expression with the corresponding SE3 displacement: change $\\ ^A\\nu$ expressed in $\\mathcal{F}_A$ into $\\ ^B\\nu$ expressed in $\\mathcal{F}_B$ is done with the so-called SE(3) **action**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aMb = pin.SE3.Random()\n",
    "anu = pin.Motion.Random()\n",
    "bnu = aMb.act(anu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SE3 action, also call \"big\" Adjoint, is a linear operation in $\\nu$, that we denote by the action matrix $\\ ^AX_B$. The action matrix can be explicited with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aXb = aMb.action\n",
    "bnu_vec = aXb @ anu.vector\n",
    "assert norm(bnu_vec-bnu.vector) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jacobians can be changed with the same way. Formally, the colums of the jacobian are spatial velocities, yet they are not represented that way in Pinocchio, and the `pin.SE3.act` function does not work on jacobian. You have to explicitly use the action matrix.\n",
    "\n",
    "For example, the jacobian of the operation frame wrt the target frame, expressed in the target frame, is $\\ ^*J$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetJ = Mtarget.inverse().action @ oJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you followed properly, you should be convinced that the jacobian corresponding to $\\ ^{target}M_{e}$ is the \n",
    "same as the one for $\\ ^oM_e$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log jacobian\n",
    "\n",
    "Now, we have two representations possible for the jacobian of $(\\ ^oM_{*}^{-1} \\ ^oM_e)$: either $\\ ^*J$ or $\\ ^eJ$. Which one should we choose. Well, actually, it depends on the choise we make for representing the jacobian of the log.\n",
    "\n",
    "M(q) is a function that maps the configuration space (a vector space, so far) into the Lie group SE(3). On the other hand, the *log* is a function thats maps the same Lie group SE(3) into the vector space $\\mathbb{R}^6$ (or, more specifically, the space of spatial velocity, which is a real vector space of dimension 6). So, similarly to the jacobian of M, the jacobian of *log* can be represented either in the frame attache to the operational frame $\\mathcal{F}_e$ or to the frame attached to the fixed target $\\mathcal{F}_*. \n",
    "\n",
    "Let's look at the documentation of `pin.Jlog6` to knwo which frame to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows `__doc__` in a jupyter notebook panel at the bottom of your screen\n",
    "pin.Jlog6?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in clear, the frame Jacobian should be expressed in the local frame, as the jacobian of the log is also expressed in this frame.\n",
    "\n",
    "We can now conclude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradrient of the cost function\n",
    "\n",
    "Our cost is $c(q) = r(q)^T r(q)$, where the residual $r(q) = log(\\ ^*M_e(q))$. \n",
    "The cost gradient is then $\\nabla c = \\frac{d c}{d q} = 2 J^T r$, where $J = \\frac{dr}{dq}$, the jacobian of the residual, is the product of the jacobian of the log $J_{log}$ and the jacobian of the operational frame placement $J_q$.\n",
    "$$ \\nabla c = 2 J_q^T J_{log}^T r(q)$$\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![question](question.png)\n",
    "Rewrite the cost 6d class of the previous notebook, with an additional `def calcDiff(self,q)` function that return the gradient of the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%do_not_load -r 47-78 costs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <img src=\"recap.png\" title=\"Recap\"/>\n",
    "    <h3>Recap of the main syntax elements exposed in this section</h3>\n",
    "    <ul>\n",
    "        <li><code>pin.computeJointJacobians(rmodel, rdata, q)</code> precomputes all the jacobians.</li>\n",
    "        <li><code>pin.getJointJacobian(rmodel, rdata, joint_index, pin.ReferenceFrame)</code> and <code>pin.getFrameJacobian(rmodel, rdata, frame_index, pin.ReferenceFrame)</code> returns the joint and frame jacobians, where <code>pin.ReferenceFrame</code> can be either <code>pin.LOCAL</code> or <code>pin.WORLD</code>. </li>\n",
    "        <li><code>pin.SE3.act</code> can change the expression frame of spatial velocities.</li>\n",
    "        <li><code>pin.SE3.action</code> is the $6\\times 6$ action matrix, that can right multiplies joint and frame jacobians.</li>\n",
    "        <li><code>pin.Jlog6</code> compute the jacobian of the log of SE(3).</li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Finite differencing for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*When you work with gradient based optimization, always start with finite differences*.  \n",
    "\n",
    "This sentence could be carved on my grave. There are several reasons to that. First, finite-differences are much easier to implement. It also implies they are less error prone. Most of the time, they work just as well, even if definitely slower. So you can prototype your mathematical program with them, and see if you missed something, at minimal cost. \n",
    "\n",
    "And to finish, you *definitely* need to validate your derivatives against finite differencing, as a basic unitary test. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<center><b>YOU<br>\n",
    "\n",
    "<center><b>DEFINITELY<br>\n",
    "\n",
    "<center><b>HAVE TO<br>\n",
    " \n",
    "<center><b>validate your derivatives against finite differences<br>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NumDiff function is simple to implement \n",
    "Here is a quick implementation of finite differencing. Use it each time you implement a new derivatives in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numdiff(func, x, eps=1e-6):\n",
    "    f0 = copy.copy(func(x))\n",
    "    xe = x.copy()\n",
    "    fs = []\n",
    "    for k in range(len(x)):\n",
    "        xe[k] += eps\n",
    "        fs.append((func(xe) - f0) / eps)\n",
    "        xe[k] -= eps\n",
    "    if isinstance(f0, np.ndarray) and len(f0) > 1:\n",
    "        return np.stack(fs,axis=1)\n",
    "    else:\n",
    "        return np.matrix(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using your derivatives inside fmin_bfgs\n",
    "`fmin_bfgs` is taking an optinal argument `fprime` that should returns an array of the same dimension than the decision variable: `fmin_bfgs(func, x0, fprime=grad_func)`, where `grad_func(x0)` has the same shape as `x0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without derivatives.\n",
    "# There are 3 times more function evaluations than grad evaluations, \n",
    "# because of internal finite differences.\n",
    "x = fmin_bfgs(np.cos, .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With derivatives.\n",
    "# There is one func call, for each fprime call.\n",
    "x = fmin_bfgs(np.cos, .1, lambda x: -np.sin(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![question](question.png)\n",
    "Validate your Cost6d.calcDiff with finite differencing, then run fmin_bfgs with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%do_not_load -r 26-28 solutions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <img src=\"recap.png\" title=\"Recap\"/>\n",
    "    <h3>Recap of the main syntax elements exposed in this section</h3>\n",
    "    <ul>\n",
    "        <li><code>assert(norm(numdiff(cost.calc, q) - cost.calcDiff(q)) < 1e-3)</code> for validating your derivatives (local unefficient implementation).</li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Derivatives the 3d \"position\" cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started with the 6d cost because 6d quantities are more logical in Pinocchio. But we now have nearly everything for differentiating the 3d cost as well. We just need to introduce the `pin.LOCAL_WORLD_ALIGNED` concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3d velocities\n",
    "\n",
    "We consider the position of a point $p$ attached to the robot, expressed in the world frame: $^op(q)$. Its time derivatives corresponds to the velocity of $p$ expressed in the world frame, i.e nothing fancy: $^o\\dot{p} = \\ ^ov_p$.\n",
    "\n",
    "Consider first that $p$ is the center of the frame $\\mathcal{F}_p$. Then the local expression of the spatial velocity of $\\mathcal{F}_p$ is $^p\\nu_p = (\\ ^pv_p,\\ ^p\\omega)$, where the linear part $^pv_p$ is the velocity of $p$, expressed in the local frame. We then have:\n",
    "\n",
    "$$^ov_p = \\ ^oR_p \\ ^pv_p = \\ ^oR_p \\ ^p\\nu.linear$$\n",
    "\n",
    "where $^oR_p$ is the rotation of $\\mathcal{F}_p$ wrt the world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local expression aligned to the world\n",
    "\n",
    "When considering spatial quantities, local or world representation are generally good enough. But when we are interrested by the particular values of its linear part, we are very often in the case described in the previous paragraph: we want the local quantity $v_p$, but we would prefer to have it along world axes, and not the local axes. So jacobians can be evaluated this way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oJp = pin.getFrameJacobian(rmodel, rdata, frame_index, pin.LOCAL_WORLD_ALIGNED)[:3, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take care**: if you want to consider the 6D jacobian, choosing this representation is very likely to mess something up, as you don't have something matching the spatial algebra anymore. But if you are only interested by the 3D part, this is the way to go. \n",
    "\n",
    "Alternatively, you can simply rotate the 3 first rows to align them from local frame to world frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oJp2 = rdata.oMf[frame_index].rotation @ pin.getFrameJacobian(rmodel, rdata, frame_index, pin.LOCAL)[:3,:]\n",
    "assert norm(oJp - oJp2) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost 3d derivative\n",
    "\n",
    "Recall first that our 3d cost is $cost(q) = r(q)^T r(q)$ with $r(q) = \\ ^op(q)- p_{target}$. \n",
    "\n",
    "![question](question.png)\n",
    "\n",
    "Implement the gradient of the 3d cost introduced in the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%do_not_load -r 10-43 costs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to test it against `numdiff`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%do_not_load -r 31-33 solutions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <img src=\"recap.png\" title=\"Recap\"/>\n",
    "    <h3>Recap of the main syntax elements exposed in this section</h3>\n",
    "    <ul>\n",
    "        <li><code>pin.LOCAL_WORLD_ALIGNED</code> produces spatial quantities expressed locally but represented along the world axes... use it with care!</li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Derivatives of the posture cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce the posture cost as simple $cost(q) = || q-q^* ||^2$, for a given reference posture $q^*$. Its gradient is straightforward ... let's make it a little bit more difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posture gradient\n",
    "\n",
    "The cost is $cost(q) = r(q)^T r(q)$ with $r(q)=q-q^*$. The gradient is simply $\\frac{d cost}{dq} = 2 r(q)$\n",
    "\n",
    "#### Posture cost, renewed\n",
    "\n",
    "When the configuration is a plain vector (i.e. not a fancy Lie element), $q-q^*$ works fine.  But we saw in the last tutorial that it does not work anymore when we have a free basis, hence a quaternion in the configuration. In that case, the residual should be computed by $r(q) = $`pin.difference(rmodel,q,qref)$.\n",
    "\n",
    "The jacobian of the `pin.difference` operation is given by `pin.dDifference`. This function actually outputs the derivatives with respect to $q$ first, and to $q^*$ second, but we only need the first one, as we consider here that $q^*$ does not vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtarget = robot.q0.copy()\n",
    "Jdiff,_ = pin.dDifference(rmodel, q, qtarget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, `Jdiff` is identity, but it would not be anymore when we will have a free basis. \n",
    "\n",
    "![question](question.png)\n",
    "Modify the cost posture introduced in the previous notebook, so that it works with `pin.difference`, and add the `CostPosture.calcDiff(self,q)` function to compute its gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%do_not_load -r 147-160 costs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to check your gradient against `numdiff`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%do_not_load -r 36-38 solutions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <img src=\"recap.png\" title=\"Recap\"/>\n",
    "    <h3>Recap of the main syntax elements exposed in this section</h3>\n",
    "    <ul>\n",
    "        <li><code>pin.dDifference(rmodel, rdata, q1, q2)</code> computes the jacobian of <code>pin.difference</code></li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Derivatives of the two gravity costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce two cost functions penalizing the gravity cost: $cost_g(q) = g(q)^T g(q)$, and $cost_{Mg}(q) = g(q)^T M(q)^{-1} g(q)$. We will see that the gradient of the first is straightforward, while the gradient of the second involves the derivatives of both RNEA and ABA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient of the gravity torque\n",
    "\n",
    "The gravity $g(q)$ is computed by `pin.computeGeneralizedGravity(rmodel, rdata, q)`. The jacobian of this function is directly implement as `pin.computeGeneralizedGravityDerivatives(rmodel, rdata, q)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = pin.computeGeneralizedGravity(rmodel, rdata, q)\n",
    "dg = pin.computeGeneralizedGravityDerivatives(rmodel, rdata, q)\n",
    "dgn = numdiff(lambda q: pin.computeGeneralizedGravity(rmodel, rdata, q), q)\n",
    "assert norm(dg - dgn) < 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the gravity cost is simple $\\frac{d cost_g}{dq} = 2 \\frac{dq}{dq}^T g(q)$.\n",
    "\n",
    "![question](question.png) \n",
    "Copy the gravity cost implemented in the previous tutorial and implement the derivatives of the gravity cost in `CostGravity.calcDiff`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%do_not_load -r 107-122 costs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget ... numdiff ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%do_not_load -r 41-43 solutions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivatives of the weighted gravity\n",
    "\n",
    "Let's recall first the RNEA and ABA functions:\n",
    "$$ rnea(q,v_q,a_q) = \\tau_q$$\n",
    "$$ aba(q,v_q,\\tau_q) = a_q$$\n",
    "The gravity torque can be computed from RNEA when $v_q=0$ and $a_q=0$: $g(q)=rnea(q,v_q=0,a_q=0)$.\n",
    "\n",
    "Then we have $g(q)^T M(q)^{-1} g(q) = rnea(q,0,0)^T aba(q,0,0)$. To compute its derivatives, the easiest is to rely on the derivatives of RNEA and ABA.\n",
    "\n",
    "The derivatives of RNEA are computed by `pin.computeRNEADerivatives`. The function computes the derivatives with respect to $q$, $v_q$ and $a_q$, i.e. produces 3 matrices. They are available in `rdata.dtau_dq` and `rdata.dtau_dv`. The derivative wrt to $a_q$ is simply $M(q)$, available in `rdata.M`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v0 = np.zeros(rmodel.nv)\n",
    "pin.computeRNEADerivatives(rmodel, rdata, q, v0, v0)\n",
    "assert norm(rdata.dtau_dq - numdiff(lambda q: pin.rnea(rmodel, rdata, q, v0, v0), q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the derivatives of ABA are computed by `pin.computeABADerivatives` and stored in `rdata.ddq_dq` and `rdata.ddq_dv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pin.computeABADerivatives(rmodel, rdata, q, np.zeros(rmodel.nv), np.zeros(rmodel.nv))\n",
    "assert norm(rdata.ddq_dq-numdiff(lambda q: pin.aba(rmodel, rdata, q, v0, v0), q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![question](question.png)\n",
    "Copy the weighted gravity cost implemented in the previous tutorial and implement the derivatives of ths cost in `CostWeightedGravity.calcDiff`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%do_not_load -r 125-143 costs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget ... numdiff ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%do_not_load -r 46-48 solutions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <img src=\"recap.png\" title=\"Recap\"/>\n",
    "    <h3>Recap of the main syntax elements exposed in this section</h3>\n",
    "    <ul>\n",
    "        <li><code>pin.computeRNEADerivatives(rmodel, rdata, q, vq, aq)</code> computes the derivatives of RNEA wrt $q$, $v_q$ and $a_q$ and stores them in <code>rdata.dtau_dq</code>, <code>rdata.dtau_dv</code>, <code>rdata.M</code>.</li>\n",
    "        <li><code>pin.computeABADerivatives(rmodel, rdata, q, vq, tauq)</code> computes the derivatives of ABA wrt $q$, $v_q$ and $\\tau_q$ and stores them in <code>rdata.ddq_dq</code>, <code>rdata.ddq_dv</code>, <code>rdata.Minv</code>.</li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The return of the free flyer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we should already be able to run your previous BFGS program with analytic derivatives. We just need one last step to be able to generalize them to a robot with a free basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot = robex.load('talos')\n",
    "#robot = robex.load('solo')\n",
    "viz = Viewer(robot.model, robot.collision_model, robot.visual_model)\n",
    "viz.initViewer(loadModel=True)\n",
    "viz.display(robot.q0)\n",
    "isinstance(viz, pin.visualize.MeshcatVisualizer) and viz.viewer.jupyter_cell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmodel = robot.model\n",
    "rdata = rmodel.createData()\n",
    "frameName = 'wrist_left_ft_tool_link' if rmodel.name == 'talos' else 'HR_KFE' \n",
    "frame_index = rmodel.getFrameId(frameName)\n",
    "joint_index = rmodel.frames[frame_index].parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jacobian and tangent application\n",
    "\n",
    "In the early sections of this tutorial, we have seen that the derivatives in Lie group can go beyond the usual definition. For example, the derivative of the placement, represented by a matrix $4\\times 4$, is in fact a vector $6$. These derivatives, that we have called *jacobians*, are indeed *tangent applications*, a notion that matches jacobians when the input and output spaces are vector spaces, but that extends it in Lie groups. To explicit this subtelty, the tangent application of $f(q)$ is sometime denoted $T_qf$, which reads: \"the tangent application of f with respect to variable $q$ computed at point $q$\" .\n",
    "\n",
    "When the robot has a free basis, its configuration vector is not a real vector any more, but encompasses a rotation, typically represented by a quaternion in Pinocchio. That $\\mathcal{Q}$ is a Lie group!\n",
    "\n",
    "The derivatives that we get with Pinocchio have the same number of columns as `rmodel.nv`. They must multiply with a velocity $v_q$, and cannot multiply with a vector differene of two configurations $\\Delta q$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = pin.randomConfiguration(rmodel)\n",
    "vq = np.random.rand(rmodel.nv) * 2 - 1\n",
    "J = pin.computeJointJacobian(rmodel, rdata, q, joint_index)\n",
    "J @ vq  # ok, cartesian velocity of dim 6\n",
    "try:\n",
    "    J @ q\n",
    "except:\n",
    "    print('!!! ERROR')\n",
    "    print('As expected, you cannot multiply J with a q')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet, the solvers from SciPy do not know anything about Lie groups. They work with $q$ as if it was a real vector. Then they expect the derivatives of the cost function as if they were *vector* derivatives. \n",
    "\n",
    "In Pinocchio, we call this **coefficient-wise** derivatives. Some coefficient-wise derivatives are implemented, but they are not yet binded in Python. For this tutorial, we propose a Python partial implementation, for the particular case of free-basis robot. Next realeses of Pinocchio would offer an extended, and more efficient solution.\n",
    "\n",
    "What we need for now is a way of transforming the tangent applications that we already computed, into coefficient-wise jacobians that SciPy is expected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coefficient-wise derivatives\n",
    "\n",
    "Let's first recall the notation introduced in the first tutorial: we denote the integration operation with $\\oplus$: $ q \\oplus v_q = $ `pin.integrate(rmodel, q, vq)`. The tangent application can indeed be defined with $\\oplus$ as:\n",
    "$$ T_qf = \\frac{\\partial f(q\\oplus v_q)}{\\partial v_q}$$\n",
    "\n",
    "By applying the chain rule, we can link the tangent application to the coefficient-wise jacobian. Let' s denote by $h(v_q) := f(q\\oplus v_q)$, and by $i(v_q):= q\\oplus v_q$. Then $h(v_q) = f \\circ i(v_q) = f(i(v_q))$. The chain rule then gives:\n",
    "\n",
    "$$ T_q f = \\frac{df}{dq} \\frac{di}{dv_q} $$\n",
    "where $T_q f$ is the tangent application that we are already computing, $\\frac{df}{dq}$ is the coefficient-wise derivative that we are looking for, and $Q := \\frac{di}{dv_q}$ is a new matrix that we need to compute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dExp and dExp_inv\n",
    "\n",
    "Actually, we need the pseudo inverse of this matrix. Both $di/dv$ and its inverse are implemented in Python in the local final dexp.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dexp import dExpQ, dExpQ_inv\n",
    "\n",
    "Q = dExpQ(rmodel,q)\n",
    "Qinv = dExpQ_inv(rmodel,q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From tangent application to coefficient-wise jacobian\n",
    "\n",
    "The tangent $T$ has one less column than the coefficient-wise jacobian $J$. This means that we can pass from $T$ to $J$, but the reciprocal cannot be done without additional prior information. Actually, we can show that $QQ^+$ is a projector onto the normal to the configuration vector, i.e. the only missing information to pass from $T$ to $J$ is \"what happen in the direction where the quaternion changes norm$. We then only have an approximation of the coefficient-wise Jacobian, but that is relevant for all the directions that matter.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the changes to the gradient algorithm\n",
    "\n",
    "The gradient that you computed up to now are indeed tangent applications, but you just have to multiply with $Q^+$ to obtain the coefficient-wise that SciPy needs. You can change all the functions above, or (more efficient and less demanding), you can simply apply this final transformation in your mixture of cost.\n",
    "\n",
    "See the appendix at the end of the tutorial if you need to assert tangent application with finite differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![question](question.png)\n",
    "\n",
    "Copy the sum of costs of the previous tutorial, and implement the `Cost.calcDiff` function by summing the tangent applications (gradients) of the cost already defined and multiplying the result with $Q^+$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%do_not_load -r 52-72 solutions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <img src=\"recap.png\" title=\"Recap\"/>\n",
    "    <h3>Recap of the main syntax elements exposed in this section</h3>\n",
    "    <ul>\n",
    "        <li><code>dExpQ(rmodel, q)</code> and <code>dExpQ_inv(rmodel, q)</code> respectively compute the exponential coefficient-wise derivative and its pseudo inverse.</li>\n",
    "        <li><code>numdiff(f, q)</code> approximates the coefficient-wise derivative of `f` wrt to `q`.</li>\n",
    "        <li><code>Tqdiff(f, q)(</code> approximates the Lie tangent application of `f` wrt `q`.</li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "### Finite differences for tangent applications\n",
    "\n",
    "If you have chosen not to modify the `Cost.calcDiff` of your main cost classes, then the `numdiff` assertions are not valid any more, as `numdiff` is approximating the coefficient-wise jacobian.\n",
    "Here is a solution: you can also approximate the tangent application by finite differences, by integrating finite steps instead of vector sums in the `numdif` routine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tdiff(func, exp, nv, q, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Generic finite-differences when the input space is a Lie group, whose integration is defined by q' = q exp(v).\n",
    "    - func is the function dy differenciate.\n",
    "    - exp is the integration, working as q2 = exp(q1,vq).\n",
    "    - nv is the size of the tangent space, i.e. size of vq.\n",
    "    - q is the point where the tangent application should be evaluated.\n",
    "    - eps is the finite-difference step.\n",
    "    \"\"\"\n",
    "    f0 = copy.copy(func(q))\n",
    "    fs = []\n",
    "    v = np.zeros(nv)\n",
    "    for k in range(nv):\n",
    "        v[k] = eps\n",
    "        qk = exp(q, v)\n",
    "        fs.append((func(qk) - f0) / eps)\n",
    "        v[k] -= eps\n",
    "    if isinstance(fs[0], np.ndarray) and len(fs[0]) > 1: \n",
    "        return np.stack(fs, axis=1)\n",
    "    else: \n",
    "        return np.array(fs)\n",
    "    \n",
    "def Tqdiff(func, q): \n",
    "    return Tdiff(func, exp=lambda q, v: pin.integrate(rmodel, q, v), nv=rmodel.nv, q=q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
